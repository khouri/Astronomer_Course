Error: error building, (re)creating or starting project containers: Error response from daemon: driver failed programming external connectivity on endpoint astro_116e0c-postgres-1 (9ead4d375a051fd93fea5307bfa2e2af291cc13691a2c7a834903bd9b88970e4): Bind for 127.0.0.1:5432 failed: port is already allocated
(base) nuvemshop@ip-192-168-0-128 astro % docker stop $(docker ps -q)

para solucionar:
docker stop $(docker ps -q)


Ultimas entradas do airflow
3803132	BR	Domain_BS2_Auto_Block	2023-10-06 12:30:10	https://stats.tiendanube.com/store/profile?store_id=3803132	

3802719	AR	Email_Change_Password	2023-10-06 7:30:36	https://stats.tiendanube.com/store/profile?store_id=3802719


BOASVINDAS10




#inicializa repositorio
astro dev init
astro dev start
astro dev stop


# restart
astro dev stop && astro dev start



# lista de providers
astro dev run providers list



# para obter versoes de providers distintos:
https://registry.astronomer.io/providers/dbt%20Cloud/versions/latest




# matar o processo docker e iniciar, isso mata o metabase do astro
astro dev kill && astro dev start


# inicializar ambiente puxando variaveis gravadas em arquivo local. Não funciona em prod é apenas local. Para producao precisa criar variaveis na UI do airflow OU dentro do Dockerfile 
astro dev start --env .dev


# checa integridade dos dags
astro dev parse



# roda comandos da pasta test
astro dev pytest



astro run my_dag -e .env|prod|dev


astro run my_dag -s another_settings_file.yaml



# Arquitetura do airflow:

webserver -> interact with the UI, task and Airflow 
			_> tem command line 
			-> tem API

Scheduler -> gerencia a execução e trigger das tasks e dags

Metastore -> banco e dados de metadados, usuários, tasks, dags, e outros

Triggerer -> permite lidar com um tipo especial de task


Executor(Local, Celery, Kubernetes) -> não executa, ele define como e qual sistema irá executar as tarefas

Queue -> a fila de execução assegura a execução, em ordem correta, de todas as tarefas

Worker -> Executa a tarefa, pode ser uma máquina ou subprocesso



Operador -> encapsula um tipo de tarefa específico (Python Operator)

	Action Operator -> executa coisas, SQL, Python, Bash
	Transfer Operator -> transfere dados de origem para destino
	Sensor -> espera algo acontecer para executar



Para interagir com outros sistemas, por ter uma natureza modular, o airflow exige a instalação de um provider (módulo que contém operadores ultra específicos) (os providers podem ser vistos aqui: https://registry.astronomer.io/)

Quando vc executa um operador (é uma task), ele vira um task instance

Um dag é um conjunto de operadores

Airflow não é uma solução de stream de dados(kafka) nem de processamento de dados (spark). Ele é um orchestrador de processos


Como o airflow funciona?
O que acontece quando vc triga um DAG ?



Summary
Thank you for joining us in this module on Airflow Concepts

We have learnt that Airflow is an open-source tool that allows users to author, schedule, and monitor workflows in data pipelines.
It is coded in Python and is scalable with a user-friendly interface.
We explored the several core components, including the web server, scheduler, meta database, triggerer, executor, queue, and worker.
We also learnt about the Directed Acyclic Graph (DAG), which is the most crucial concept, and it represents a data pipeline with nodes as tasks and directed edges as dependencies.
Moreover, the Operators are objects that encapsulate tasks, and there are three types of operators: action, transfer, and sensor operators. Providers are packages that contain operators for interacting with specific tools.
Airflow works by triggering data pipelines through the scheduler, which creates a DAGRun object and a task instance object for the first task. The task instance is then pushed into a queue and executed by the executor.
To create a DAG in Airflow, create a file in the "dags/" folder, instantiate the DAG object with parameters such as the unique DAG ID, start date, scheduling interval, and catchup parameter. Once these parameters are defined, tasks can be implemented within the DAG.
To create a task, look up the appropriate operator in the registry.astronomer.io and define the task ID and parameters needed for the operator.
Airflow is useful for scheduling batch jobs, training machine learning models, and building ETL or ELT pipelines. It saves time by better scheduling and monitoring data pipelines. However, it is not a data streaming solution or a data processing framework, but an orchestrator for batch processing jobs.
We also learnt how to define dependencies in Airflow is simple using the right and left bitshift operators, which can be seen in the Airflow UI. Dependencies can be defined between tasks, such as "start >> end" meaning "end" is executed after "start".
Thank you for tuning in. See you in the next one.


Resources
Follow the Links below for more info about the respective Modules

→ DAG Scheduling - https://academy.astronomer.io/astro-runtime-scheduling









Summary
Thank you for joining us in this module on Airflow UI

We explored the Airflow UI, where Airflow's DAGs View is the first page users see upon logging in, providing a comprehensive list of all the data pipelines in the Airflow instance.
This view displays various columns, including DAG ID, tags, scheduling interval, previous and current DAG Run statuses, most recent task states, and actions to delete or trigger the DAG.
Different views, such as the grid, graph, calendar, landing time, Gantt, and code views, are available for each DAG, allowing users to monitor and manage their DAG Runs and tasks.
The grid view presents a history of task, and DAG Run states for a particular DAG, while the graph view visually represents task dependencies.
The calendar view helps identify patterns in DAG Runs, and the landing time view helps optimize task completion times.
The Gantt view helps identify bottlenecks and latency between tasks.
The code view allows users to access the serialized code of the data pipeline stored in the database, verifying if modifications made to the pipeline are being used by the scheduler.
In case of task failure, users can go to the grid or graph view, access the failed task logs, fix the error, and rerun the task.
Users can also access the list of all DAG Runs or task instances in the Airflow instance by going to Browse and then DAG Runs or Task Instances.
Users can filter the list by adding filters such as DAG ID and select the DAG Runs or task instances they want to rerun or delete.
We also learned that it is recommended to manually delete metadata such as DAG Runs and task instances every 28 days to avoid affecting the scheduler.
Thank you for tuning in. See you in the next one.



Go further
Follow the Links below for more info about the respective Modules

→ Dag Scheduling - https://academy.astronomer.io/astro-runtime-scheduling

→ TaskGroups - https://academy.astronomer.io/astro-runtime-task-groups



Key takeaways
Landing times are calculated from the task scheduled time to the time the task finishes (end_time - scheduled_time)
Landing times correspond to the actual execution time of your tasks.
Task durations (the other view) show the total task durations (scheduled time included)
This view is perfect for evaluating the effectiveness of your changes.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#landing-times



Key takeaways
The Gantt chart is perfect for identifying task bottlenecks and overlaps.
A rectangle is a task. The longer the rectangle, the longer it took to complete the task.
A rectangle is divided into two parts. The first part (grey) is the queued time. The second is the execution time.
The queued time corresponds to the time spent waiting for a worker to pick your task. A worker executes tasks.
Two rectangles side by side mean tasks have been executed in parallel and this is an overlap.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#task-instances
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#gantt-chart


Key takeaways
The Code view shows the DAG code parsed by the Scheduler.
The Code view is perfect for checking whether or not DAG updates have been picked up.
Look at the Parsed at date instead of searching for your modification in the code.
You can't edit DAG code in the Code view.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#code-view
https://docs.astronomer.io/learn/airflow-ui#code




Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
From "Browse", you can have lists of DAG runs, Task instances, Jobs, and more.
Use "Browse" to simultaneously take actions on multiple DAG runs or Task instances.
You can search on specific objects by mixing different filters.
You can rerun a set of task instances of DAG runs by filtering on the start and end dates. Then select Clear the state.
"Browse" provides further details on your Task instances or DAG runs, such as, run type, start date, duration, etc.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#browse-tab



Key takeaways
Use tags to categorize or organize your data pipelines.
"Runs" shows the status of the past and most recent DAG runs. Whereas "Recent Tasks" shows only the status of the task instances for the active or most recent DAG runs.
"Last run" is the data interval start of the last DAG run
"Next run" is the data interval start of the next DAG run
The "Delete DAG" button doesn't delete the DAG file, only the metadata related to the DAG.
Don't forget to remove any applied filters or you may wonder why a DAG doesn't show up on the UI. It's an easy mistake.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#dags



Key takeaways
The grid view shows previous and current DAG runs with their task instances and states.
The top bars are the DAG runs. The squares are the task instances.
The longer a top bar is, the longer it took to complete that specific DAG run.
You get a DAG runs summary when you land on the Grid view. Mean run duration helps define the DAG's timeout.
Share information with the rest of your team using notes. That applies to DAG runs and task instances.
Clear restarts the selected DAG run or task instance.
When you want to clear a task instance, not including Downstream means downstream tasks to the selected task won't be rerun.
Use shortcuts if you... want? 🥹
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#grid-view
https://docs.astronomer.io/learn/airflow-ui#grid-view




Key takeaways
The Graph view is perfect for checking dependencies between tasks of a DAG.
A rectangle is a task with information such as the operator and the state.
Use the mini-map for navigating a large data pipeline with many tasks.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#graph
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#graph-view




Key takeaways
The Calendar view overviews your entire DAG's history over months or even years.
The Calendar view is perfect to spot breaking patterns or trends.
A square is a day, and the color of that square depends on the ratio between successful and failed DAG runs.
Squares with dots indicate that DAG runs are planned for these days.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#calendar-view


Wrap Up
Well done!
Here are the key takeaways from this module:

A DAG must have a unique identifier and a start_date with a datetime object
The schedule interval is optional and defines the trigger frequency of the DAG
Defining a description, the catchup parameter to avoid running past non-triggered DAG runs, and tags to filter is strongly recommended.
To create a task, look at the https://registry.astronomer.io/ first.
A task must have a unique identifier within a DAG
You can specify default parameters to all tasks with default_args that expects a dictionary
Define dependencies with bitshift operators (>> and <<) as well as lists.
chain helps to define dependencies between task lists



Summary
Thank you for joining us in this module on DAG scheduling.

We learnt about the states of a DAGRun which initially is Queued, and changes to Running as soon as the first task runs. Once all tasks are completed, the final state of the DAGRun is determined by the state of the leaf tasks, which could be Success or Failure.
The start date and scheduling interval are two important parameters to consider when scheduling a DAG. DAGRuns are created based on these parameters and have a data interval start and end corresponding to the time frame for which the DAG was executed.
The start date parameter defines the timestamp from which the scheduler will attempt to backfill, and the scheduling interval determines how often a DAG runs.
The catchup mechanism in Airflow allows running all non-triggered DAGRuns between the start date and the last time the DAG was triggered. The backfilling mechanism allows running historical DAGRuns or rerun already existing DAGRuns.
We also explored the Airflow CLI commands to backfill our DAG for any data intervals, regardless of the start date of our data pipeline.
Users can define the scheduling interval using CRON expressions or timedelta object, and can use a timetable to schedule a DAG more precisely.
It is important to set the start date parameter properly to avoid confusion with Airflow, and use a static date.
Thank you for tuning in. See you in the next one.




Introduction
→ In this activity we shall learn how to export our Connection

→ Often we might need to migrate our connections from one airflow environment to another.

→ Exporting your connections and importing them to the new environment comes in handy.

Prerequisites
→ Astro cli setup with a project

→ A Connection created on the Airflow UI


Let’s Go!


→ Part 1 - Exporting Connections

In order to export the connection from the database into a file we shall use the airflow cli.

First let’s start a bash session into our scheduler using the astro dev bash command.



Next we shall use the airflow cli to export the connection to a file, using airflow connections export connections.json



We can verify the contents using cat connections.json



You may also export the connections in either the yaml or an environment variable format using the commands:
airflow connections export --file-format yaml -



airflow connections export --file-format env -



The commands above will display the contents on the terminal. If you wish them to be stored in a file, you might as well give it a file name.

→ Part 2 - Importing Connections

Now we are ready to import these variables into our new airflow instance. However for this exercise we shall test this by deleting our connections from the UI and importing it back.
Once you delete all the connections from the UI. Verify it doesn’t exist by starting a bash session and then listing the connections using the airflow cli as follows

astro dev bash

airflow connections list




Now let’s import the connections using the command airflow connections import connections.json




Finally we can verify these on the UI, exactly the way it was.




You may also verify if it works using our existing DAG.

Well done we have managed to export and import our connections successfully!!!

(Optional)


If you wish to transfer these files to another host, you’ll have to copy these connection files to your host system.

Copy the container id of the scheduler.
Exit from the container.
Execute the command to copy the files from the container to your host system docker cp 067e047a39e2:/usr/local/airflow/connections.json ./
Verify that the file was copied to your host machine using ls.



Summary
Thank you for joining us in this module on Airflow Connections.


We learnt in order to interact with external systems in a data pipeline, one needs to create a connection, which is a set of parameters, identified by a unique connection ID, where the connection contains login, password, hostname, etc., required for accessing external systems in a data pipeline.
We also learnt that one can interact with APIs, Snowflake, DBT etc. using connections in Airflow. To create a connection to interact with an API, one probably may need to get an API key and specify the connection ID, type, description, host, and API key.
We also explored that the created connections can be listed, interacted with, and filtered from the connections view in Airflow. However if connections were defined using environment variables, they won't be visible on the UI, but they exist and can be exported using the Airflow CLI.
We also interacted with Snowflake from a data pipeline using the Snowflake operator in Airflow which involved creating a task with the Snowflake operator, creating a connection with Snowflake, and specifying the SQL request.
The Astro CLI provides the airflow_settings.yaml file, which allows for configuring and programmatically creating airflow connections, pools, and variables. The file can be used to define connection parameters such as connection ID, type, host, login, password, and port for various data sources such as Postgres. This saves time in recreating connections in the Airflow UI every time the project is restarted. However, this functionality is only available locally.
Overall, creating connections is essential for accessing external systems in a data pipeline. It enables the user to interact with APIs, Snowflake, and DBT etc. and it can be done using the connections view in Airflow or by programmatically creating connections using the Astro/Airflow CLI.
Thank you for tuning in. See you in the next one.



 docker cp cea54b99a5ce:/usr/local/airflow/connections.json ./



 Summary
Thank you for joining us in this module on XCOMs

We learnt about XCOMs, a native feature of Airflow, that enables the sharing of data between tasks by using the Airflow meta database.
To use XCOM, a DAG and task must first be created, and then an XCOM can be created by returning a value from a task that can be retrieved by another task.
The XCOM has a key, value, timestamp, task ID, and DAG ID, and must be JSON serializable.
Using the xcom_push and xcom_pull methods in Airflow, one can define a specific key to an XCOM, giving flexibility in defining task IDs and allowing for pulling data from multiple tasks simultaneously.
The Airflow UI can display XCOMs and their properties, and XCOMs can be used to share small amounts of metadata between tasks.
However, XCOMs have limitations based on the database used, with SQLite allowing up to 2GB, Postgres up to 1GB, and MySQL up to 64KB.
It is not suitable for sharing large amounts of data, and for that, one should trigger a Spark job or similar.
In summary, XCOMs are a useful feature of Apache Airflow for sharing small amounts of data between tasks, but it is important to understand their limitations and use them accordingly.
Thank you for tuning in. See you in the next one.


Summary
Thank you for joining us in this module on Airflow Variables

We learnt how the use of variables in programming can simplify and streamline code by storing information that can be used across multiple tasks or functions. Variables consist of a unique identifier, a value that is JSON serializable.
They can be created, edited, deleted, or exported and can also be stored in a secret backend or by using environment variables.
For sensitive values, Airflow allows for the creation of hidden variables by specifying certain keywords such as "api_key", "password", or "secret" in the key. These variables remain hidden on the Airflow UI even when edited, providing a safe way to store sensitive information.
In addition, Airflow variables can be created by exporting environment variables with the prefix "AIRFLOW_VAR". This method allows for keeping sensitive values hidden from the Airflow UI, faster processing, and easy versioning of variables as they are stored in a file.
While these variables cannot be seen on the Airflow UI, they can still be used in DAGs.
Overall, the use of variables in Airflow can simplify code, improve efficiency, and provide a safe way to store sensitive information.
Thank you for tuning in. See you in the next one.


Resources
Follow the Links below for more info about the respective Modules

→ Secrets Backend - https://academy.astronomer.io/astro-module-secrets-backend


Basic Checks
Notes:
DAGs don't appear in the Airflow UI - Basic Checks 📖

Before diving into debugging it is always a good idea to check whether we have got our basic right.

Often after endless debugging we find that we’ve made a tiny mistake that is too basic to think of while thinking about finding the bugs.

Here are a few basic checks to ensure that debugging your DAGs is actually required.

Ensure that all your DAG files are located in the dags folder.
The .airflowignore file does not have the name of your DAG file as Airflow ignores any files present on it.
At the code level, ensure that each DAG:
Has a unique dag_id. if two dags have the same id, Airflow randomly parses one of them.
Contains either the word airflow or dag. The scheduler only scans files that meet this requirement.
If you are using Airflow decorator to instantiate your dag with the @dag decorator, make sure  the decorated function is invoked at the end.
Further Reading 📖

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#airflowignore
https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#declaring-a-dag
https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#loading-dags




Notes:
DAGs not running properly - Common Issues

If your DAGs are not running or not behaving as expected, it may be worthwhile to investigate the following frequently encountered issues:

Refraining from immediately triggering DAGs after making changes to them or any other files in the DAG folder is advisable, as the scheduler may still need to parse the DAG.
Confirm that you have unpaused your DAGs to enable them to execute according to their schedule.
Ensure that the start_date of your DAG is set to a date in the past else if you trigger the DAG manually you will see a successful DAG run but no successful tasks.


Ensure the end_date of your DAG is set to the future else you won’t see any tasks executed like above.
Often if you expect many instances of your DAG or tasks to be running simultaneously, make sure you verify these core airflow settings usually found in airflow.cfg file
 Note:  You cannot bring your own airflow.cfg in Astronomer. However, you may configure these by declaring the respective environment variables.[4]

        → max_active_runs_per_dag (The maximum number of active DAG runs per DAG). Default → 16.

        → max_active_tasks_per_dag (The maximum number of task instances allowed to run concurrently in each DAG). Default → 16.

        → parallelism (This defines the maximum number of task instances that can run concurrently per scheduler in Airflow, regardless of the worker count). Default → 32.

The times used in your code should be in UTC for compatibility with external systems as well.
Further Reading 📖


https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html
https://docs.astronomer.io/learn/airflow-scaling-workers
https://docs.astronomer.io/learn/scheduling-in-airflow
https://docs.astronomer.io/astro/environment-variables




DAGs not running properly - Dependency Conflicts

Using different dependencies than your Airflow environment is a frequent occurrence. Your task may require a specific Python version that differs from the core Airflow, or it may have packages that clash with other tasks.

To handle these situations, running tasks in a separate environment can be beneficial as it can effectively manage dependency conflicts and ensure compatibility with your execution environments.

Airflow offers several alternatives for executing tasks in separate environments:

→ One option is the KubernetesPodOperator, which is suitable for users who operate Airflow on Kubernetes and require greater control over the resources and infrastructure used to run the task, in addition to package management. However, there are some drawbacks, such as a more complicated setup and increased task latency.

→ The ExternalPythonOperator is another choice that enables you to execute certain tasks with a different set of Python libraries than others and the primary Airflow environment. This may be a virtual environment or any pre-installed Python installation that is accessible in the Airflow task's execution environment.

→ Another option is the PythonVirtualenvOperator, which functions similarly as the ExternalPythonOperator . However, it generates and deletes a new virtual environment for each task. This operator is ideal if you don't want to retain your virtual environment. The disadvantage of this operator is that it takes more time to generate the environment each time the task is executed, resulting in higher task latency.



Too overwhelming?
Don’t worry we have separate modules that cover these topics in detail. Don’t forget to visit them.



Further Reading 📖

https://docs.astronomer.io/learn/external-python-operator
https://docs.astronomer.io/learn/kubepod-operator


Summary
Thank you for joining us in this module on debugging DAGS in Airflow.
We have learned some valuable techniques that will help you identify and resolve issues that may arise when working with Airflow.
Firstly, we looked at the basic checks that we can perform on our Airflow settings such as checking the wait intervals and the scaling parameters for our DAG.
We then learned how to validate that our DAGs and modules are present at the correct location and our DAG has been instantiated properly and that any dependencies are resolved correctly.
Next, we explored how to check the health and logs of our scheduler to make sure our DAGs have been parsed and are being scheduled properly.
We also learned how to verify the integrity of external systems that our DAGs may interact with, such as databases, and discussed the necessity to verify whether these systems allow connections to be established.
Finally, we also looked at how to resolve dependency conflicts between various DAGs by using operators such as the KubernetesPodOperator, the ExternalPythonOperator, or the PythonVirtualEnvironment operator. These three operators allow us to isolate our DAG and any of its dependencies from our main Airflow to avoid any conflicts that may arise.

With these techniques, we can ensure that our DAGs run smoothly and that any issues that may arise can be quickly identified and resolved.
Thank you for tuning in and happy debugging.
What's a Sensor?
A Sensor is a particular operator that waits for a condition to be true. If the condition is true, the task is marked successful , and the next task runs. If the condition is false, the sensor waits for another interval until it times out and fails.


In the example above, the Sensor checks  _condition to be true every 60 seconds by default (poke_interval).
Since _condition always returns False, the Sensor will continue checking every 60 seconds until
it times out after 7 days (7 * 24 * 60 * 60 by default). When the Sensor times out, it is marked failed.
That's it.
Now that you know what Sensors are, let's discover how do they work and why they are so useful.




