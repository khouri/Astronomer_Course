

Ultimas entradas do airflow
3803132	BR	Domain_BS2_Auto_Block	2023-10-06 12:30:10	https://stats.tiendanube.com/store/profile?store_id=3803132	

3802719	AR	Email_Change_Password	2023-10-06 7:30:36	https://stats.tiendanube.com/store/profile?store_id=3802719


BOASVINDAS10




#inicializa repositorio
astro dev init
astro dev start
astro dev stop


# restart
astro dev stop && astro dev start



# lista de providers
astro dev run providers list



# para obter versoes de providers distintos:
https://registry.astronomer.io/providers/dbt%20Cloud/versions/latest




# matar o processo docker e iniciar, isso mata o metabase do astro
astro dev kill && astro dev start


# inicializar ambiente puxando variaveis gravadas em arquivo local. NÃ£o funciona em prod Ã© apenas local. Para producao precisa criar variaveis na UI do airflow OU dentro do Dockerfile 
astro dev start --env .dev


# checa integridade dos dags
astro dev parse



# roda comandos da pasta test
astro dev pytest



astro run my_dag -e .env|prod|dev


astro run my_dag -s another_settings_file.yaml



# Arquitetura do airflow:

webserver -> interact with the UI, task and Airflow 
			_> tem command line 
			-> tem API

Scheduler -> gerencia a execuÃ§Ã£o e trigger das tasks e dags

Metastore -> banco e dados de metadados, usuÃ¡rios, tasks, dags, e outros

Triggerer -> permite lidar com um tipo especial de task


Executor(Local, Celery, Kubernetes) -> nÃ£o executa, ele define como e qual sistema irÃ¡ executar as tarefas

Queue -> a fila de execuÃ§Ã£o assegura a execuÃ§Ã£o, em ordem correta, de todas as tarefas

Worker -> Executa a tarefa, pode ser uma mÃ¡quina ou subprocesso



Operador -> encapsula um tipo de tarefa especÃ­fico (Python Operator)

	Action Operator -> executa coisas, SQL, Python, Bash
	Transfer Operator -> transfere dados de origem para destino
	Sensor -> espera algo acontecer para executar



Para interagir com outros sistemas, por ter uma natureza modular, o airflow exige a instalaÃ§Ã£o de um provider (mÃ³dulo que contÃ©m operadores ultra especÃ­ficos) (os providers podem ser vistos aqui: https://registry.astronomer.io/)

Quando vc executa um operador (Ã© uma task), ele vira um task instance

Um dag Ã© um conjunto de operadores

Airflow nÃ£o Ã© uma soluÃ§Ã£o de stream de dados(kafka) nem de processamento de dados (spark). Ele Ã© um orchestrador de processos


Como o airflow funciona?
O que acontece quando vc triga um DAG ?



Summary
Thank you for joining us in this module on Airflow Concepts

We have learnt that Airflow is an open-source tool that allows users to author, schedule, and monitor workflows in data pipelines.
It is coded in Python and is scalable with a user-friendly interface.
We explored the several core components, including the web server, scheduler, meta database, triggerer, executor, queue, and worker.
We also learnt about the Directed Acyclic Graph (DAG), which is the most crucial concept, and it represents a data pipeline with nodes as tasks and directed edges as dependencies.
Moreover, the Operators are objects that encapsulate tasks, and there are three types of operators: action, transfer, and sensor operators. Providers are packages that contain operators for interacting with specific tools.
Airflow works by triggering data pipelines through the scheduler, which creates a DAGRun object and a task instance object for the first task. The task instance is then pushed into a queue and executed by the executor.
To create a DAG in Airflow, create a file in the "dags/" folder, instantiate the DAG object with parameters such as the unique DAG ID, start date, scheduling interval, and catchup parameter. Once these parameters are defined, tasks can be implemented within the DAG.
To create a task, look up the appropriate operator in the registry.astronomer.io and define the task ID and parameters needed for the operator.
Airflow is useful for scheduling batch jobs, training machine learning models, and building ETL or ELT pipelines. It saves time by better scheduling and monitoring data pipelines. However, it is not a data streaming solution or a data processing framework, but an orchestrator for batch processing jobs.
We also learnt how to define dependencies in Airflow is simple using the right and left bitshift operators, which can be seen in the Airflow UI. Dependencies can be defined between tasks, such as "start >> end" meaning "end" is executed after "start".
Thank you for tuning in. See you in the next one.


Resources
Follow the Links below for more info about the respective Modules

â†’ DAG Scheduling - https://academy.astronomer.io/astro-runtime-scheduling









Summary
Thank you for joining us in this module on Airflow UI

We explored the Airflow UI, where Airflow's DAGs View is the first page users see upon logging in, providing a comprehensive list of all the data pipelines in the Airflow instance.
This view displays various columns, including DAG ID, tags, scheduling interval, previous and current DAG Run statuses, most recent task states, and actions to delete or trigger the DAG.
Different views, such as the grid, graph, calendar, landing time, Gantt, and code views, are available for each DAG, allowing users to monitor and manage their DAG Runs and tasks.
The grid view presents a history of task, and DAG Run states for a particular DAG, while the graph view visually represents task dependencies.
The calendar view helps identify patterns in DAG Runs, and the landing time view helps optimize task completion times.
The Gantt view helps identify bottlenecks and latency between tasks.
The code view allows users to access the serialized code of the data pipeline stored in the database, verifying if modifications made to the pipeline are being used by the scheduler.
In case of task failure, users can go to the grid or graph view, access the failed task logs, fix the error, and rerun the task.
Users can also access the list of all DAG Runs or task instances in the Airflow instance by going to Browse and then DAG Runs or Task Instances.
Users can filter the list by adding filters such as DAG ID and select the DAG Runs or task instances they want to rerun or delete.
We also learned that it is recommended to manually delete metadata such as DAG Runs and task instances every 28 days to avoid affecting the scheduler.
Thank you for tuning in. See you in the next one.



Go further
Follow the Links below for more info about the respective Modules

â†’ Dag Scheduling - https://academy.astronomer.io/astro-runtime-scheduling

â†’ TaskGroups - https://academy.astronomer.io/astro-runtime-task-groups



Key takeaways
Landing times are calculated from the task scheduled time to the time the task finishes (end_time - scheduled_time)
Landing times correspond to the actual execution time of your tasks.
Task durations (the other view) show the total task durations (scheduled time included)
This view is perfect for evaluating the effectiveness of your changes.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#landing-times



Key takeaways
The Gantt chart is perfect for identifying task bottlenecks and overlaps.
A rectangle is a task. The longer the rectangle, the longer it took to complete the task.
A rectangle is divided into two parts. The first part (grey) is the queued time. The second is the execution time.
The queued time corresponds to the time spent waiting for a worker to pick your task. A worker executes tasks.
Two rectangles side by side mean tasks have been executed in parallel and this is an overlap.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#task-instances
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#gantt-chart


Key takeaways
The Code view shows the DAG code parsed by the Scheduler.
The Code view is perfect for checking whether or not DAG updates have been picked up.
Look at the Parsed at date instead of searching for your modification in the code.
You can't edit DAG code in the Code view.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#code-view
https://docs.astronomer.io/learn/airflow-ui#code




Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
From "Browse", you can have lists of DAG runs, Task instances, Jobs, and more.
Use "Browse" to simultaneously take actions on multiple DAG runs or Task instances.
You can search on specific objects by mixing different filters.
You can rerun a set of task instances of DAG runs by filtering on the start and end dates. Then select Clear the state.
"Browse" provides further details on your Task instances or DAG runs, such as, run type, start date, duration, etc.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#browse-tab



Key takeaways
Use tags to categorize or organize your data pipelines.
"Runs" shows the status of the past and most recent DAG runs. Whereas "Recent Tasks" shows only the status of the task instances for the active or most recent DAG runs.
"Last run" is the data interval start of the last DAG run
"Next run" is the data interval start of the next DAG run
The "Delete DAG" button doesn't delete the DAG file, only the metadata related to the DAG.
Don't forget to remove any applied filters or you may wonder why a DAG doesn't show up on the UI. It's an easy mistake.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#dags



Key takeaways
The grid view shows previous and current DAG runs with their task instances and states.
The top bars are the DAG runs. The squares are the task instances.
The longer a top bar is, the longer it took to complete that specific DAG run.
You get a DAG runs summary when you land on the Grid view. Mean run duration helps define the DAG's timeout.
Share information with the rest of your team using notes. That applies to DAG runs and task instances.
Clear restarts the selected DAG run or task instance.
When you want to clear a task instance, not including Downstream means downstream tasks to the selected task won't be rerun.
Use shortcuts if you... want? ğŸ¥¹
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#grid-view
https://docs.astronomer.io/learn/airflow-ui#grid-view




Key takeaways
The Graph view is perfect for checking dependencies between tasks of a DAG.
A rectangle is a task with information such as the operator and the state.
Use the mini-map for navigating a large data pipeline with many tasks.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#graph
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#graph-view




Key takeaways
The Calendar view overviews your entire DAG's history over months or even years.
The Calendar view is perfect to spot breaking patterns or trends.
A square is a day, and the color of that square depends on the ratio between successful and failed DAG runs.
Squares with dots indicate that DAG runs are planned for these days.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#calendar-view


Wrap Up
Well done!
Here are the key takeaways from this module:

A DAG must have a unique identifier and a start_date with a datetime object
The schedule interval is optional and defines the trigger frequency of the DAG
Defining a description, the catchup parameter to avoid running past non-triggered DAG runs, and tags to filter is strongly recommended.
To create a task, look at the https://registry.astronomer.io/ first.
A task must have a unique identifier within a DAG
You can specify default parameters to all tasks with default_args that expects a dictionary
Define dependencies with bitshift operators (>> and <<) as well as lists.
chain helps to define dependencies between task lists