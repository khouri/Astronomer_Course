

Ultimas entradas do airflow
3803132	BR	Domain_BS2_Auto_Block	2023-10-06 12:30:10	https://stats.tiendanube.com/store/profile?store_id=3803132	

3802719	AR	Email_Change_Password	2023-10-06 7:30:36	https://stats.tiendanube.com/store/profile?store_id=3802719


BOASVINDAS10




#inicializa repositorio
astro dev init
astro dev start
astro dev stop


# restart
astro dev stop && astro dev start



# lista de providers
astro dev run providers list



# para obter versoes de providers distintos:
https://registry.astronomer.io/providers/dbt%20Cloud/versions/latest




# matar o processo docker e iniciar, isso mata o metabase do astro
astro dev kill && astro dev start


# inicializar ambiente puxando variaveis gravadas em arquivo local. Não funciona em prod é apenas local. Para producao precisa criar variaveis na UI do airflow OU dentro do Dockerfile 
astro dev start --env .dev


# checa integridade dos dags
astro dev parse



# roda comandos da pasta test
astro dev pytest



astro run my_dag -e .env|prod|dev


astro run my_dag -s another_settings_file.yaml



# Arquitetura do airflow:

webserver -> interact with the UI, task and Airflow 
			_> tem command line 
			-> tem API

Scheduler -> gerencia a execução e trigger das tasks e dags

Metastore -> banco e dados de metadados, usuários, tasks, dags, e outros

Triggerer -> permite lidar com um tipo especial de task


Executor(Local, Celery, Kubernetes) -> não executa, ele define como e qual sistema irá executar as tarefas

Queue -> a fila de execução assegura a execução, em ordem correta, de todas as tarefas

Worker -> Executa a tarefa, pode ser uma máquina ou subprocesso



Operador -> encapsula um tipo de tarefa específico (Python Operator)

	Action Operator -> executa coisas, SQL, Python, Bash
	Transfer Operator -> transfere dados de origem para destino
	Sensor -> espera algo acontecer para executar



Para interagir com outros sistemas, por ter uma natureza modular, o airflow exige a instalação de um provider (módulo que contém operadores ultra específicos) (os providers podem ser vistos aqui: https://registry.astronomer.io/)

Quando vc executa um operador (é uma task), ele vira um task instance

Um dag é um conjunto de operadores

Airflow não é uma solução de stream de dados(kafka) nem de processamento de dados (spark). Ele é um orchestrador de processos


Como o airflow funciona?
O que acontece quando vc triga um DAG ?



Summary
Thank you for joining us in this module on Airflow Concepts

We have learnt that Airflow is an open-source tool that allows users to author, schedule, and monitor workflows in data pipelines.
It is coded in Python and is scalable with a user-friendly interface.
We explored the several core components, including the web server, scheduler, meta database, triggerer, executor, queue, and worker.
We also learnt about the Directed Acyclic Graph (DAG), which is the most crucial concept, and it represents a data pipeline with nodes as tasks and directed edges as dependencies.
Moreover, the Operators are objects that encapsulate tasks, and there are three types of operators: action, transfer, and sensor operators. Providers are packages that contain operators for interacting with specific tools.
Airflow works by triggering data pipelines through the scheduler, which creates a DAGRun object and a task instance object for the first task. The task instance is then pushed into a queue and executed by the executor.
To create a DAG in Airflow, create a file in the "dags/" folder, instantiate the DAG object with parameters such as the unique DAG ID, start date, scheduling interval, and catchup parameter. Once these parameters are defined, tasks can be implemented within the DAG.
To create a task, look up the appropriate operator in the registry.astronomer.io and define the task ID and parameters needed for the operator.
Airflow is useful for scheduling batch jobs, training machine learning models, and building ETL or ELT pipelines. It saves time by better scheduling and monitoring data pipelines. However, it is not a data streaming solution or a data processing framework, but an orchestrator for batch processing jobs.
We also learnt how to define dependencies in Airflow is simple using the right and left bitshift operators, which can be seen in the Airflow UI. Dependencies can be defined between tasks, such as "start >> end" meaning "end" is executed after "start".
Thank you for tuning in. See you in the next one.


Resources
Follow the Links below for more info about the respective Modules

→ DAG Scheduling - https://academy.astronomer.io/astro-runtime-scheduling









Summary
Thank you for joining us in this module on Airflow UI

We explored the Airflow UI, where Airflow's DAGs View is the first page users see upon logging in, providing a comprehensive list of all the data pipelines in the Airflow instance.
This view displays various columns, including DAG ID, tags, scheduling interval, previous and current DAG Run statuses, most recent task states, and actions to delete or trigger the DAG.
Different views, such as the grid, graph, calendar, landing time, Gantt, and code views, are available for each DAG, allowing users to monitor and manage their DAG Runs and tasks.
The grid view presents a history of task, and DAG Run states for a particular DAG, while the graph view visually represents task dependencies.
The calendar view helps identify patterns in DAG Runs, and the landing time view helps optimize task completion times.
The Gantt view helps identify bottlenecks and latency between tasks.
The code view allows users to access the serialized code of the data pipeline stored in the database, verifying if modifications made to the pipeline are being used by the scheduler.
In case of task failure, users can go to the grid or graph view, access the failed task logs, fix the error, and rerun the task.
Users can also access the list of all DAG Runs or task instances in the Airflow instance by going to Browse and then DAG Runs or Task Instances.
Users can filter the list by adding filters such as DAG ID and select the DAG Runs or task instances they want to rerun or delete.
We also learned that it is recommended to manually delete metadata such as DAG Runs and task instances every 28 days to avoid affecting the scheduler.
Thank you for tuning in. See you in the next one.



Go further
Follow the Links below for more info about the respective Modules

→ Dag Scheduling - https://academy.astronomer.io/astro-runtime-scheduling

→ TaskGroups - https://academy.astronomer.io/astro-runtime-task-groups



Key takeaways
Landing times are calculated from the task scheduled time to the time the task finishes (end_time - scheduled_time)
Landing times correspond to the actual execution time of your tasks.
Task durations (the other view) show the total task durations (scheduled time included)
This view is perfect for evaluating the effectiveness of your changes.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#landing-times



Key takeaways
The Gantt chart is perfect for identifying task bottlenecks and overlaps.
A rectangle is a task. The longer the rectangle, the longer it took to complete the task.
A rectangle is divided into two parts. The first part (grey) is the queued time. The second is the execution time.
The queued time corresponds to the time spent waiting for a worker to pick your task. A worker executes tasks.
Two rectangles side by side mean tasks have been executed in parallel and this is an overlap.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#task-instances
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#gantt-chart


Key takeaways
The Code view shows the DAG code parsed by the Scheduler.
The Code view is perfect for checking whether or not DAG updates have been picked up.
Look at the Parsed at date instead of searching for your modification in the code.
You can't edit DAG code in the Code view.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#code-view
https://docs.astronomer.io/learn/airflow-ui#code




Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
A DAG run fails when a task fails in it.
The first step to debug a task instance is to look into the logs.
You get different logs for each retry (attempt) to the same task.
Pause the DAG before fixing the error. Otherwise, your DAG runs will continue to fail.
Retry multiple DAG Runs and Task instances at once using Browse.
Additional resources
Links that can help you:

Airflow: Debug DAGs module
Debug DAGs - Astronomer doc



Key takeaways
From "Browse", you can have lists of DAG runs, Task instances, Jobs, and more.
Use "Browse" to simultaneously take actions on multiple DAG runs or Task instances.
You can search on specific objects by mixing different filters.
You can rerun a set of task instances of DAG runs by filtering on the start and end dates. Then select Clear the state.
"Browse" provides further details on your Task instances or DAG runs, such as, run type, start date, duration, etc.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#browse-tab



Key takeaways
Use tags to categorize or organize your data pipelines.
"Runs" shows the status of the past and most recent DAG runs. Whereas "Recent Tasks" shows only the status of the task instances for the active or most recent DAG runs.
"Last run" is the data interval start of the last DAG run
"Next run" is the data interval start of the next DAG run
The "Delete DAG" button doesn't delete the DAG file, only the metadata related to the DAG.
Don't forget to remove any applied filters or you may wonder why a DAG doesn't show up on the UI. It's an easy mistake.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#dags



Key takeaways
The grid view shows previous and current DAG runs with their task instances and states.
The top bars are the DAG runs. The squares are the task instances.
The longer a top bar is, the longer it took to complete that specific DAG run.
You get a DAG runs summary when you land on the Grid view. Mean run duration helps define the DAG's timeout.
Share information with the rest of your team using notes. That applies to DAG runs and task instances.
Clear restarts the selected DAG run or task instance.
When you want to clear a task instance, not including Downstream means downstream tasks to the selected task won't be rerun.
Use shortcuts if you... want? 🥹
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#grid-view
https://docs.astronomer.io/learn/airflow-ui#grid-view




Key takeaways
The Graph view is perfect for checking dependencies between tasks of a DAG.
A rectangle is a task with information such as the operator and the state.
Use the mini-map for navigating a large data pipeline with many tasks.
Additional resources
Links that can help you:

https://docs.astronomer.io/learn/airflow-ui#graph
https://airflow.apache.org/docs/apache-airflow/stable/ui.html#graph-view




Key takeaways
The Calendar view overviews your entire DAG's history over months or even years.
The Calendar view is perfect to spot breaking patterns or trends.
A square is a day, and the color of that square depends on the ratio between successful and failed DAG runs.
Squares with dots indicate that DAG runs are planned for these days.
Additional resources
Links that can help you:

https://airflow.apache.org/docs/apache-airflow/stable/ui.html#calendar-view


Wrap Up
Well done!
Here are the key takeaways from this module:

A DAG must have a unique identifier and a start_date with a datetime object
The schedule interval is optional and defines the trigger frequency of the DAG
Defining a description, the catchup parameter to avoid running past non-triggered DAG runs, and tags to filter is strongly recommended.
To create a task, look at the https://registry.astronomer.io/ first.
A task must have a unique identifier within a DAG
You can specify default parameters to all tasks with default_args that expects a dictionary
Define dependencies with bitshift operators (>> and <<) as well as lists.
chain helps to define dependencies between task lists



Summary
Thank you for joining us in this module on DAG scheduling.

We learnt about the states of a DAGRun which initially is Queued, and changes to Running as soon as the first task runs. Once all tasks are completed, the final state of the DAGRun is determined by the state of the leaf tasks, which could be Success or Failure.
The start date and scheduling interval are two important parameters to consider when scheduling a DAG. DAGRuns are created based on these parameters and have a data interval start and end corresponding to the time frame for which the DAG was executed.
The start date parameter defines the timestamp from which the scheduler will attempt to backfill, and the scheduling interval determines how often a DAG runs.
The catchup mechanism in Airflow allows running all non-triggered DAGRuns between the start date and the last time the DAG was triggered. The backfilling mechanism allows running historical DAGRuns or rerun already existing DAGRuns.
We also explored the Airflow CLI commands to backfill our DAG for any data intervals, regardless of the start date of our data pipeline.
Users can define the scheduling interval using CRON expressions or timedelta object, and can use a timetable to schedule a DAG more precisely.
It is important to set the start date parameter properly to avoid confusion with Airflow, and use a static date.
Thank you for tuning in. See you in the next one.




Introduction
→ In this activity we shall learn how to export our Connection

→ Often we might need to migrate our connections from one airflow environment to another.

→ Exporting your connections and importing them to the new environment comes in handy.

Prerequisites
→ Astro cli setup with a project

→ A Connection created on the Airflow UI


Let’s Go!


→ Part 1 - Exporting Connections

In order to export the connection from the database into a file we shall use the airflow cli.

First let’s start a bash session into our scheduler using the astro dev bash command.



Next we shall use the airflow cli to export the connection to a file, using airflow connections export connections.json



We can verify the contents using cat connections.json



You may also export the connections in either the yaml or an environment variable format using the commands:
airflow connections export --file-format yaml -



airflow connections export --file-format env -



The commands above will display the contents on the terminal. If you wish them to be stored in a file, you might as well give it a file name.

→ Part 2 - Importing Connections

Now we are ready to import these variables into our new airflow instance. However for this exercise we shall test this by deleting our connections from the UI and importing it back.
Once you delete all the connections from the UI. Verify it doesn’t exist by starting a bash session and then listing the connections using the airflow cli as follows

astro dev bash

airflow connections list




Now let’s import the connections using the command airflow connections import connections.json




Finally we can verify these on the UI, exactly the way it was.




You may also verify if it works using our existing DAG.

Well done we have managed to export and import our connections successfully!!!

(Optional)


If you wish to transfer these files to another host, you’ll have to copy these connection files to your host system.

Copy the container id of the scheduler.
Exit from the container.
Execute the command to copy the files from the container to your host system docker cp 067e047a39e2:/usr/local/airflow/connections.json ./
Verify that the file was copied to your host machine using ls.



Summary
Thank you for joining us in this module on Airflow Connections.


We learnt in order to interact with external systems in a data pipeline, one needs to create a connection, which is a set of parameters, identified by a unique connection ID, where the connection contains login, password, hostname, etc., required for accessing external systems in a data pipeline.
We also learnt that one can interact with APIs, Snowflake, DBT etc. using connections in Airflow. To create a connection to interact with an API, one probably may need to get an API key and specify the connection ID, type, description, host, and API key.
We also explored that the created connections can be listed, interacted with, and filtered from the connections view in Airflow. However if connections were defined using environment variables, they won't be visible on the UI, but they exist and can be exported using the Airflow CLI.
We also interacted with Snowflake from a data pipeline using the Snowflake operator in Airflow which involved creating a task with the Snowflake operator, creating a connection with Snowflake, and specifying the SQL request.
The Astro CLI provides the airflow_settings.yaml file, which allows for configuring and programmatically creating airflow connections, pools, and variables. The file can be used to define connection parameters such as connection ID, type, host, login, password, and port for various data sources such as Postgres. This saves time in recreating connections in the Airflow UI every time the project is restarted. However, this functionality is only available locally.
Overall, creating connections is essential for accessing external systems in a data pipeline. It enables the user to interact with APIs, Snowflake, and DBT etc. and it can be done using the connections view in Airflow or by programmatically creating connections using the Astro/Airflow CLI.
Thank you for tuning in. See you in the next one.



 docker cp cea54b99a5ce:/usr/local/airflow/connections.json ./



 Summary
Thank you for joining us in this module on XCOMs

We learnt about XCOMs, a native feature of Airflow, that enables the sharing of data between tasks by using the Airflow meta database.
To use XCOM, a DAG and task must first be created, and then an XCOM can be created by returning a value from a task that can be retrieved by another task.
The XCOM has a key, value, timestamp, task ID, and DAG ID, and must be JSON serializable.
Using the xcom_push and xcom_pull methods in Airflow, one can define a specific key to an XCOM, giving flexibility in defining task IDs and allowing for pulling data from multiple tasks simultaneously.
The Airflow UI can display XCOMs and their properties, and XCOMs can be used to share small amounts of metadata between tasks.
However, XCOMs have limitations based on the database used, with SQLite allowing up to 2GB, Postgres up to 1GB, and MySQL up to 64KB.
It is not suitable for sharing large amounts of data, and for that, one should trigger a Spark job or similar.
In summary, XCOMs are a useful feature of Apache Airflow for sharing small amounts of data between tasks, but it is important to understand their limitations and use them accordingly.
Thank you for tuning in. See you in the next one.


Summary
Thank you for joining us in this module on Airflow Variables

We learnt how the use of variables in programming can simplify and streamline code by storing information that can be used across multiple tasks or functions. Variables consist of a unique identifier, a value that is JSON serializable.
They can be created, edited, deleted, or exported and can also be stored in a secret backend or by using environment variables.
For sensitive values, Airflow allows for the creation of hidden variables by specifying certain keywords such as "api_key", "password", or "secret" in the key. These variables remain hidden on the Airflow UI even when edited, providing a safe way to store sensitive information.
In addition, Airflow variables can be created by exporting environment variables with the prefix "AIRFLOW_VAR". This method allows for keeping sensitive values hidden from the Airflow UI, faster processing, and easy versioning of variables as they are stored in a file.
While these variables cannot be seen on the Airflow UI, they can still be used in DAGs.
Overall, the use of variables in Airflow can simplify code, improve efficiency, and provide a safe way to store sensitive information.
Thank you for tuning in. See you in the next one.


Resources
Follow the Links below for more info about the respective Modules

→ Secrets Backend - https://academy.astronomer.io/astro-module-secrets-backend